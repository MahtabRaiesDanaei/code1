
from google.colab import drive
drive.mount('/content/drive')





!pip install https://github.com/pybrain/pybrain/archive/0.3.3.zip

!pip install tensorflow==2.3.0
!pip install gym
!pip install keras
!pip install keras-rl2


!pip install keras==2.11.0

!pip install tensorflow==2.11.0

import numpy as np
from gym import Env
from gym.spaces import Box , Discrete
import random
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense , Flatten
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
from rl.agents import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

class customEnv(Env):
  def __init__(self):
    self.action_space=Discrete(3)#sample 0 1 2
    self.observation_space=Box(low=np.array([0]) , high=np.array([100]))#scope that tempurature could be
    self.state=38+ random.randint(-3,3)
    self.seconds=60


  def step(self,action):          #action 0=D 1=S 2=U

    self.state += action -1       #38+(0-1)=37 down shode  reward=-1
    self.seconds-=1

    if self.state >=37 and self.state<= 39:
      reward=1
    else:
      reward=-1

    if self.seconds<=0:
      done=True
    else :
      done=False
    info={}
    return self.state , reward , done , info


  def reset(self):
    self.state=38+ random.randint(-3,3)
    self.seconds=60
    return self.state

env=customEnv()

episode=20

for ep in range(episode):
  state=env.reset() # give us new state
  done=False
  score=0

  while not done:
    action=env.action_space.sample()
    sate ,reward ,done , info =env.step(action)
    score+=reward

states=env.observation_space.shape #38
actions=env.action_space.n # 0 1 2
def my_model(states,actions):
  model=Sequential()
  model.add(Dense(24,activation='relu',input_shape=states))
  model.add(Dense(24,activation='relu'))
  model.add(Dense(actions,activation='linear'))
  return model

model=my_model(states , actions)
model.summary()

def my_agent(my_model,actions):
  policy=BoltzmannQPolicy()
  memory=SequentialMemory(limit=50000,window_length=1)
  dqn=DQNAgent(model=my_model,memory=memory,policy=policy,nb_actions=actions,nb_steps_warmup=10,target_model_update=1e-2)
  return dqn


mahtab=my_agent(model,actions)
mahtab.compile( tf.keras.optimizers.legacy.Adam(learning_rate=1e-3), metrics=['mae'])
mahtab.fit(env,nb_steps=60000,verbose=1)

import gym
import numpy as np

gym.envs.registry.all()

env=gym.make('FrozenLake-v1')

action_size=env.action_space.n #4 --> left R U D
observation_size=env.observation_space.n # 16 --> 16 cell
qtable=np.zeros((observation_size,action_size))
qtable

num_episodes=10000 # from S to G
num_steps=100    # number of actions it can have in one episode
lr=0.8 #learning rate
gamma=0.9

for episode in range(num_episodes):
  state =env.reset()
  all_rewards=0

  for step in range(num_steps):
    if step==0:
      action=env.action_space.sample() #random 0 1 2 3
    else:
      action=np.argmax(qtable[state,:]) #max qtable in a row
    newstate , reward , done , info = env.step(action)
    qtable[state,action]=qtable[state,action]+lr*(reward + gamma* np.max(qtable[newstate,:])-qtable[state,action])
    all_rewards+=reward
    state=newstate
    if done==True:
      break

print(qtable)



for episode in range(10):
  state=env.reset()
  print('episode:', episode)
  for step in range(100):
    env.render(mode='rgb_array')
    action=np.argmax(qtable[state,:])
    newstate , reward, done , info =env.step(action)
    state=newstate
    if done==True:
      break
#env.close()
